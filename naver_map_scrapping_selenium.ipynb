{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097d017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc25976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "애월읍 고내리의 데이터는 45개 입니다\n",
      "애월읍 고성리의 데이터는 9개 입니다\n",
      "애월읍 곽지리의 데이터는 42개 입니다\n",
      "애월읍 광령리의 데이터는 45개 입니다\n",
      "애월읍 구엄리의 데이터는 28개 입니다\n",
      "애월읍 금성리의 데이터는 9개 입니다\n",
      "애월읍 납읍리의 데이터는 4개 입니다\n",
      "애월읍 봉성리의 데이터는 15개 입니다\n",
      "애월읍 상가리의 데이터는 8개 입니다\n",
      "애월읍 상귀리의 데이터는 7개 입니다\n",
      "애월읍 소길리의 데이터는 8개 입니다\n",
      "애월읍 수산리의 데이터는 1개 입니다\n",
      "애월읍 신엄리의 데이터는 38개 입니다\n",
      "애월읍 애월리의 데이터는 127개 입니다\n",
      "애월읍 어음리의 데이터는 9개 입니다\n",
      "애월읍 유수암리의 데이터는 38개 입니다\n",
      "애월읍 장전리의 데이터는 10개 입니다\n",
      "애월읍 하가리의 데이터는 17개 입니다\n",
      "애월읍 하귀1리의 데이터는 96개 입니다\n",
      "애월읍 하귀2리의 데이터는 94개 입니다\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 애월읍 음식점 \n",
    "aewol = ['고내리', '고성리', '곽지리', '광령리', '구엄리', '금성리', '납읍리', '봉성리', '상가리', '상귀리', '소길리', '수산리', '신엄리', '애월리', '어음리', '유수암리', '장전리', '하가리', '하귀1리', '하귀2리']\n",
    "# aewol = ['고성리']\n",
    "eup = '애월읍'\n",
    "\n",
    "\n",
    "for ri in aewol : \n",
    "    url = f\"https://map.naver.com/v5/search/{eup}+{ri}+음식점?c=14060546.9964590,3952164.0916793,9,0,0,0,dh\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, \"searchIframe\")))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 페이지번호를 위한 파싱\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "    # 몇번 돌지 정하기 \n",
    "    a = soup.select('div._2ky45 > a ')\n",
    "    a = [int(i.text) for i in a[1:-1]]\n",
    "    b = max(a)\n",
    "\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for _ in range(b) :\n",
    "\n",
    "        for i in range(5):\n",
    "            scroll = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]')))\n",
    "            driver.execute_script(\"arguments[0].scrollBy(0,arguments[0].scrollHeight)\", scroll)  \n",
    "            time.sleep(3)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        naver = soup.select('#_pcmap_list_scroll_container > ul > li')\n",
    "\n",
    "        for store in naver:\n",
    "            try: \n",
    "                name = store.select(\" div >  span._3Yilt \")[0].text.strip() \n",
    "                try :\n",
    "                    point = store.select('em')[0].text\n",
    "\n",
    "                except : \n",
    "                    point = \"\"\n",
    "\n",
    "                review = store.select('div._2nqHm')[0].text\n",
    "                review = review.replace(\",\", \"\")\n",
    "\n",
    "                data.append([name, point, review])\n",
    "            except :\n",
    "                pass\n",
    "\n",
    "        driver.find_element_by_css_selector(f'#app-root > div > div._2lx2y > div._2ky45 > a:nth-child({b+2}) > svg').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['name', 'point', 'review']) \n",
    "    df['visit_review'] = df.review.str.extract(r'방문자리뷰\\s(\\d*)')\n",
    "    df['blog_review'] = df.review.str.extract(r'블로그리뷰\\s(\\d*)')\n",
    "    df = df.drop('review', axis = 1)\n",
    "    \n",
    "    \n",
    "    # 카페데이터 불러오기 \n",
    "    url = f\"https://map.naver.com/v5/search/{eup}+{ri}+카페?c=14060546.9964590,3952164.0916793,9,0,0,0,dh\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, \"searchIframe\")))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 페이지번호를 위한 파싱\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "    # 몇번 돌지 정하기 \n",
    "    try :\n",
    "        a = soup.select('div._2ky45 > a ')\n",
    "        a = [int(i.text) for i in a[1:-1]]\n",
    "        b = max(a)\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    caffe = []\n",
    "    for _ in range(b) :\n",
    "        for i in range(10):\n",
    "            scroll = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]')))\n",
    "            driver.execute_script(\"arguments[0].scrollBy(0,arguments[0].scrollHeight)\", scroll)  \n",
    "            time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        naver = soup.select('#_pcmap_list_scroll_container > ul > li')\n",
    "\n",
    "        \n",
    "        for store in naver:\n",
    "            try: \n",
    "                name = store.select(\" div > div > span._3Apve\")[0].text.strip() \n",
    "                caffe.append(name)\n",
    "            except :\n",
    "                pass\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(f'#app-root > div > div._2lx2y > div._2ky45 > a:nth-child({b+2}) > svg').click()\n",
    "            time.sleep(1)\n",
    "        except :\n",
    "            pass\n",
    "        \n",
    "    # 중복제거하기 \n",
    "    result = df[~df.name.isin(caffe)]\n",
    "    print(f'{eup} {ri}의 데이터는 {result.name.count()}개 입니다')\n",
    "    result.to_excel(f'naver_{eup}_{ri}.xlsx', index=False)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00282f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "243\n",
      "성산읍 고성리의 데이터는 205개 입니다\n",
      "13\n",
      "성산읍 난산리의 데이터는 9개 입니다\n",
      "19\n",
      "성산읍 삼달리의 데이터는 13개 입니다\n",
      "50\n",
      "100\n",
      "148\n",
      "180\n",
      "성산읍 성산리의 데이터는 144개 입니다\n",
      "33\n",
      "성산읍 수산리의 데이터는 23개 입니다\n",
      "15\n",
      "성산읍 시흥리의 데이터는 11개 입니다\n",
      "22\n",
      "성산읍 신산리의 데이터는 13개 입니다\n",
      "9\n",
      "성산읍 신천리의 데이터는 6개 입니다\n",
      "7\n",
      "성산읍 신풍리의 데이터는 3개 입니다\n",
      "39\n",
      "성산읍 오조리의 데이터는 27개 입니다\n",
      "40\n",
      "성산읍 온평리의 데이터는 35개 입니다\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 성산읍 음식점 \n",
    "sungsan = ['고성리', '난산리', '삼달리', '성산리', '수산리', '시흥리', '신산리', '신천리', '신풍리', '오조리', '온평리']\n",
    "# sungsan = ['고성리']\n",
    "eup = '성산읍'\n",
    "\n",
    "\n",
    "\n",
    "for ri in sungsan : \n",
    "    url = f\"https://map.naver.com/v5/search/{eup}+{ri}+음식점?c=14060546.9964590,3952164.0916793,9,0,0,0,dh\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, \"searchIframe\")))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 페이지번호를 위한 파싱\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "    # 몇번 돌지 정하기 \n",
    "    a = soup.select('div._2ky45 > a ')\n",
    "    a = [int(i.text) for i in a[1:-1]]\n",
    "    b = max(a)\n",
    "\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for _ in range(b) :\n",
    "\n",
    "        for i in range(10):\n",
    "            scroll = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]')))\n",
    "            driver.execute_script(\"arguments[0].scrollBy(0,arguments[0].scrollHeight)\", scroll)  \n",
    "            time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        naver = soup.select('#_pcmap_list_scroll_container > ul > li')\n",
    "\n",
    "        for store in naver:\n",
    "            try: \n",
    "                name = store.select(\" div._1z7ih > div >  span._3Yilt \")[0].text.strip() \n",
    "#                 name = store.select('div.Ow5Yt > a:nth-child(1) > div ')[0].text.strip()\n",
    "               \n",
    "                try :\n",
    "                    point = store.select('em')[0].text\n",
    "\n",
    "                except : \n",
    "                    point = \"\"\n",
    "\n",
    "                review = store.select('div._2nqHm')[0].text\n",
    "                review = review.replace(\",\", \"\")\n",
    "\n",
    "                data.append([name, point, review])\n",
    "            except :\n",
    "                pass\n",
    "        print(len(data))\n",
    "        driver.find_element_by_css_selector(f'#app-root > div > div._2lx2y > div._2ky45 > a:nth-child({b+2}) > svg').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns = ['name', 'point', 'review']) \n",
    "    df['visit_review'] = df.review.str.extract(r'방문자리뷰\\s(\\d*)')\n",
    "    df['blog_review'] = df.review.str.extract(r'블로그리뷰\\s(\\d*)')\n",
    "    df = df.drop('review', axis = 1)\n",
    "    \n",
    "    \n",
    "    # 카페데이터 불러오기 \n",
    "    url = f\"https://map.naver.com/v5/search/{eup}+{ri}+카페?c=14060546.9964590,3952164.0916793,9,0,0,0,dh\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, \"searchIframe\")))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 페이지번호를 위한 파싱\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "    # 몇번 돌지 정하기 \n",
    "    try :\n",
    "        a = soup.select('div._2ky45 > a ')\n",
    "        a = [int(i.text) for i in a[1:-1]]\n",
    "        b = max(a)\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    caffe = []\n",
    "    for _ in range(b) :\n",
    "        for i in range(10):\n",
    "            scroll = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]')))\n",
    "            driver.execute_script(\"arguments[0].scrollBy(0,arguments[0].scrollHeight)\", scroll)  \n",
    "            time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        naver = soup.select('#_pcmap_list_scroll_container > ul > li')\n",
    "\n",
    "        \n",
    "        for store in naver:\n",
    "            try: \n",
    "                name = store.select(\" div > div > span._3Apve\")[0].text.strip() \n",
    "                caffe.append(name)\n",
    "            except :\n",
    "                pass\n",
    "        try:\n",
    "            driver.find_element_by_css_selector(f'#app-root > div > div._2lx2y > div._2ky45 > a:nth-child({b+2}) > svg').click()\n",
    "            time.sleep(1)\n",
    "        except :\n",
    "            pass\n",
    "        \n",
    "    # 중복제거하기 \n",
    "    result = df[~df.name.isin(caffe)]\n",
    "    print(f'{eup} {ri}의 데이터는 {result.name.count()}개 입니다')\n",
    "    result.to_excel(f'naver_{eup}_{ri}.xlsx', index=False)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c90002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
